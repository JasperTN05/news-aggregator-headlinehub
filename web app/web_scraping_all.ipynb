{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f385dab1",
   "metadata": {},
   "source": [
    "## Web Scraping Script for all 20 Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809c500",
   "metadata": {},
   "source": [
    "This Script first gets the link for the newest articles of a website and then builds a DataFrame with the most important features of each article.\n",
    "\n",
    "The News Site used are:\n",
    "- CNN\n",
    "- FOX NEWS\n",
    "- ABC NEWS\n",
    "- CNBC NEWS\n",
    "- Politico\n",
    "- Washington Post\n",
    "- New York Times\n",
    "- MSNBC\n",
    "- Reuters\n",
    "- USA Today\n",
    "- Bloomberg\n",
    "- CBS News\n",
    "- The Wall Street Journal\n",
    "- Los Angeles Times\n",
    "- Chicago Tribune\n",
    "- HuffPost\n",
    "- Al Jazeera English\n",
    "- Time\n",
    "- NPR\n",
    "- BBC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbedad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ad7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_html_tags(html_text):\n",
    "    # Use BeautifulSoup to parse the HTML content\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "    # Extract text content without HTML tags\n",
    "    text_content = soup.get_text()\n",
    "\n",
    "    # Remove extra whitespaces and newline characters\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text_content).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "def remove_words(input_string, words_to_remove):\n",
    "    return ' '.join([word for word in input_string.split() if word not in words_to_remove])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e838d",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39784095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links for all articles\n",
    "def links_cnn():\n",
    "    index = 0\n",
    "    dict_cnn = {}  \n",
    "    links = []\n",
    "    driver = webdriver.Edge()\n",
    "    for i in range(1):\n",
    "        url = f\"https://edition.cnn.com/search?q=&from={i * 100}&size=100&page={i}&sort=newest&types=article&section=\"\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        links_on_page = soup.find_all(\"span\", attrs = {\"class\":\"container__headline-text\"})\n",
    "        links.extend(links_on_page)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    for i in range(len(links)):\n",
    "        link = links[i][\"data-zjs-href\"]\n",
    "        dict_cnn[index]= link\n",
    "        index +=1\n",
    "    return dict_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e299a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the features for one article\n",
    "def attributes_cnn(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    response = requests.get(article)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    content = soup.find_all(\"p\", attrs = {\"class\": \"paragraph inline-placeholder\"})\n",
    "    # definitions\n",
    "    text = []\n",
    "    new_dict= {}\n",
    "    text_count = {}\n",
    "    # text and lenght\n",
    "    for paragraph in content:\n",
    "        y = paragraph.get_text().strip()\n",
    "        text.append(y)\n",
    "    text =\"\".join(text)\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    try:\n",
    "        title = soup.find(\"h1\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "    try:   \n",
    "        date = soup.find(\"div\", attrs = {\"class\":\"timestamp\"}).get_text().strip()\n",
    "        date = date.split(\"     \")[-1]\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "    try:\n",
    "        reading_time = soup.find(\"div\", attrs = {\"class\": \"headline__sub-description\"}).get_text().strip()\n",
    "        reading_time = reading_time.split(\" \")[0]\n",
    "    except AttributeError:\n",
    "        reading_time = None\n",
    "    try:\n",
    "        author = soup.find(\"span\", attrs = {\"class\": \"byline__name\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        author = None  \n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[0][\"src\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "        \n",
    "    url = article\n",
    "    \n",
    "    x = (article.split(\".com/\")[1])\n",
    "    x = x.split(\"/\")\n",
    "    if x[0]==\"2023\":\n",
    "        category = x[3]\n",
    "    else:\n",
    "        category = x[0]\n",
    "\n",
    "    new_dict[0]={\"Source\":\"CNN\", \"Title\": title, \"Date\" : date,\"Author\" : author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : url, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d465eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a DatFrame with all articles\n",
    "def cnn_df():\n",
    "    dict_cnn = links_cnn()\n",
    "    dataframes_CNN = []\n",
    "    for url in dict_cnn.keys():\n",
    "        df = attributes_cnn(dict_cnn[url])\n",
    "        dataframes_CNN.append(df)\n",
    "\n",
    "    cnn_df = pd.concat(dataframes_CNN, ignore_index=True)\n",
    "    cnn_df[\"Date\"] = cnn_df[\"Date\"].apply(lambda x : parser.parse(x) if pd.notnull(x) else None)\n",
    "    cnn_df[\"Date\"].dropna(inplace=True)\n",
    "    cnn_df[\"Date\"] = cnn_df[\"Date\"].astype(str)\n",
    "    cnn_df[\"Date\"] = cnn_df[\"Date\"].apply(lambda x : x.split(\".\")[0] if pd.notnull(x) else None)\n",
    "    cnn_df = cnn_df[cnn_df[\"Text lenght\"]>100]\n",
    "    return cnn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34349d1",
   "metadata": {},
   "source": [
    "## FOX NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8975ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_fox():\n",
    "    index = 0\n",
    "    dict_fox = {}  \n",
    "    urls = 'https://www.foxnews.com/search-results/search?q=news'\n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(urls)\n",
    "    time.sleep(60)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    items = soup.find_all(\"h2\", attrs = {\"class\":\"title\"})\n",
    "    items = items[2:]\n",
    "    for i in range(len(items)):\n",
    "        dict_fox[index]= items[i].find_all(\"a\")[0][\"href\"]\n",
    "        index +=1\n",
    "    driver.quit()\n",
    "    return dict_fox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edad85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_fox(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    response = requests.get(article)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    content = soup.find_all(\"p\")\n",
    "    # definitions\n",
    "    text = []\n",
    "    new_dict= {}\n",
    "    text_count = {}\n",
    "    # text and lenght\n",
    "    for paragraph in content:\n",
    "        y = paragraph.get_text().strip()\n",
    "        text.append(y)\n",
    "    text =\"\".join(text[2:-6])\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    try:\n",
    "        title = soup.find(\"h1\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "        \n",
    "    try:   \n",
    "        date = soup.find(\"span\", attrs = {\"class\":\"article-date\"}).get_text().strip()\n",
    "        date = date.split(\"     \")[-1]\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "    source = \"Fox News\"\n",
    "    url = article\n",
    "    try:\n",
    "        x = soup.find_all(\"div\", attrs={\"class\":\"author-byline\"})\n",
    "        author = x[0].get_text().strip()\n",
    "        author = author.split(\"\\n\")[1]\n",
    "        author = author.split(\"Fox\")[0].strip()\n",
    "    except IndexError:\n",
    "        author = None\n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[0][\"src\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "\n",
    "    category = article.split(\"/\")[3]\n",
    "    new_dict[0]={\"Source\":source, \"Title\":title, \"Date\" : date,\"Author\":author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : url, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9548d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fox_df():\n",
    "    dict_fox = links_fox()\n",
    "    dataframes_FOX = []\n",
    "    for url in dict_fox.keys():\n",
    "        df = attributes_fox(dict_fox[url])\n",
    "        dataframes_FOX.append(df)\n",
    "\n",
    "    fox_df = pd.concat(dataframes_FOX, ignore_index=True)\n",
    "    fox_df[\"Date\"] = fox_df[\"Date\"].apply(lambda x : parser.parse(x) if pd.notnull(x) else None)\n",
    "    fox_df[\"Date\"].dropna(inplace=True)\n",
    "    fox_df[\"Date\"] = fox_df[\"Date\"].astype(str)\n",
    "    fox_df[\"Date\"] = fox_df[\"Date\"].apply(lambda x : x.split(\".\")[0] if pd.notnull(x) else None)\n",
    "    fox_df = fox_df[fox_df[\"Text lenght\"]>100]\n",
    "    return fox_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d7a1d",
   "metadata": {},
   "source": [
    "## ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16eddb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    index = 0\n",
    "    dict_abc = {}\n",
    "    driver = webdriver.Edge()\n",
    "\n",
    "    for i in range(10):\n",
    "        url = f\"https://abcnews.go.com/search?searchtext=news&sort=date&page={i}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html)\n",
    "        section = soup.find(\"section\", {\"class\":\"ContentRoll\"})\n",
    "        links = section.find_all(\"a\", {\"class\":\"AnchorLink\"})\n",
    "        for i in links:\n",
    "            dict_abc[index]= i[\"href\"]\n",
    "            index +=1\n",
    "\n",
    "    driver.quit()\n",
    "except Exception:\n",
    "        index = 0\n",
    "        dict_abc = {}\n",
    "        driver = webdriver.Edge()\n",
    "\n",
    "        for i in range(10):\n",
    "            url = f\"https://abcnews.go.com/search?searchtext=news&sort=date&page={i}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html)\n",
    "            section = soup.find(\"section\", {\"class\":\"ContentRoll\"})\n",
    "            links = section.find_all(\"a\", {\"class\":\"AnchorLink\"})\n",
    "            for i in links:\n",
    "                dict_abc[index]= i[\"href\"]\n",
    "                index +=1\n",
    "\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5695be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links for all articles\n",
    "def links_abc():\n",
    "    index = 0\n",
    "    dict_abc = {}\n",
    "    driver = webdriver.Edge()\n",
    "\n",
    "    for i in range(2):\n",
    "        url = f\"https://abcnews.go.com/search?searchtext=news&sort=date&page={i}\"\n",
    "        driver.get(url)\n",
    "        html = driver.page_source\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(html)\n",
    "        section = soup.find(\"section\", {\"class\":\"ContentRoll\"})\n",
    "        links = section.find_all(\"a\", {\"class\":\"AnchorLink\"})\n",
    "        for i in links:\n",
    "            dict_abc[index]= i[\"href\"]\n",
    "            index +=1\n",
    "\n",
    "    driver.quit()\n",
    "    return dict_abc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e598f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the features for one article\n",
    "def attributes_abc(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    # definitions\n",
    "    text = []\n",
    "    new_dict= {}\n",
    "    text_count = {}\n",
    "    \n",
    "    \n",
    "    response = requests.get(article)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    content = soup.find_all(\"p\")\n",
    "\n",
    "    # text and lenght\n",
    "    for paragraph in content[1:-2]:\n",
    "        y = paragraph.get_text().strip()\n",
    "        text.append(y)\n",
    "    text =\"\".join(text)\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"h1\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "    \n",
    "    x = soup.find_all(\"div\")\n",
    "    if x==[]:\n",
    "        author = None\n",
    "        date = None\n",
    "    for i in x:\n",
    "        try:\n",
    "            if i[\"data-testid\"] == \"prism-byline\":\n",
    "                info = i.get_text().strip()\n",
    "                author = info\n",
    "                date = info\n",
    "                break\n",
    "        except (AttributeError, KeyError):\n",
    "            author = None\n",
    "            date = None \n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[1][\"src\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "\n",
    "        \n",
    "    category = article.split(\".com\")[1].split(\"/\")[1]\n",
    "\n",
    "    new_dict[0]={\"Source\":\"ABC\",\"Title\": title, \"Date\" : date,\"Author\" : author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : article, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1278523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(row):\n",
    "    if row[\"Date\"]==None:\n",
    "        row[\"Author\"] = None\n",
    "        row[\"Date\"] = None\n",
    "    elif \"December\" in row[\"Date\"]:\n",
    "        row[\"Author\"] = row[\"Author\"].split(\"December\")[0].split(\"By\")[1].strip()\n",
    "        row[\"Date\"] = f\"December {row['Date'].split('December')[1]}\".strip()\n",
    "    else:\n",
    "        row[\"Author\"] = None\n",
    "        row[\"Date\"] = None\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24264aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc_df(dict_abc):\n",
    "    dataframes_ABC = []\n",
    "    for url in dict_abc.keys():\n",
    "        df = attributes_abc(dict_abc[url])\n",
    "        dataframes_ABC.append(df)\n",
    "\n",
    "    abc_df = pd.concat(dataframes_ABC, ignore_index=True)\n",
    "    abc_df[\"Date\"].dropna(inplace=True)\n",
    "    abc_df = abc_df.apply(clean_date, axis=1)\n",
    "    abc_df[\"Date\"].dropna(inplace=True)\n",
    "    abc_df[\"Date\"] = abc_df[\"Date\"].apply(lambda x : parser.parse(x) if pd.notnull(x) else None)\n",
    "    abc_df[\"Date\"].dropna(inplace=True)\n",
    "    abc_df[\"Date\"] = abc_df[\"Date\"].astype(str)\n",
    "    abc_df[\"Date\"] = abc_df[\"Date\"].apply(lambda x : x.split(\".\")[0] if pd.notnull(x) else None)\n",
    "    abc_df = abc_df[abc_df[\"Text lenght\"]>100]    \n",
    "    return abc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19815f97",
   "metadata": {},
   "source": [
    "## Politico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa8a02a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links for all articles\n",
    "def links_politico():\n",
    "    index = 0\n",
    "    dict_politico = {} \n",
    "    for i in range(1,6):\n",
    "        url = f\"https://www.politico.com/search/{i}?q=news&adv=true&c=a9d449a5-b61d-32fc-b70c-e15227dcdca7\"\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        sections = soup.find_all(\"div\", {\"class\":\"summary\"})\n",
    "        for i in range(len(sections)):\n",
    "            link = sections[i].find(\"a\")[\"href\"]\n",
    "            if \"subscriber\" not in link:\n",
    "                dict_politico[index] = link\n",
    "                index +=1\n",
    "    return dict_politico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db805bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the features for one article\n",
    "def attributes_politico(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    # definitions\n",
    "    text = []\n",
    "    new_dict= {}\n",
    "    text_count = {}\n",
    "    \n",
    "    r = requests.get(article)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    content = soup.find_all(\"p\", {\"class\":\"story-text__paragraph\"})\n",
    "    \n",
    "    # text and lenght\n",
    "    for i in content:\n",
    "        y = i.get_text().strip()\n",
    "        text.append(y)\n",
    "    text =\"\".join(text)\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"h2\", {\"class\":\"headline\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "        \n",
    "    try:\n",
    "        date = soup.find(\"time\").get_text()\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "    \n",
    "    try:\n",
    "        author = soup.find(\"span\", {\"class\":\"vcard\"}).get_text()\n",
    "    except AttributeError:\n",
    "        author = None\n",
    "    try:\n",
    "        category = soup.find(\"p\", {\"class\":\"category\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        category = None\n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[0][\"src\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "\n",
    "    new_dict[0]={\"Source\":\"Politico\",\"Title\": title, \"Date\" : date,\"Author\" : author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : article, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fb3c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def politico_df():\n",
    "    dict_politico = links_politico()\n",
    "    dataframes_politico = []\n",
    "    for url in dict_politico.keys():\n",
    "        df = attributes_politico(dict_politico[url])\n",
    "        dataframes_politico.append(df)\n",
    "\n",
    "    politico_df = pd.concat(dataframes_politico, ignore_index=True)\n",
    "    politico_df[\"Date\"] = politico_df[\"Date\"].apply(lambda x : parser.parse(x) if pd.notnull(x) else None)\n",
    "    politico_df[\"Date\"].dropna(inplace=True)\n",
    "    politico_df[\"Date\"] = politico_df[\"Date\"].astype(str)\n",
    "    politico_df[\"Date\"] = politico_df[\"Date\"].apply(lambda x : x.split(\".\")[0] if pd.notnull(x) else None)\n",
    "    politico_df = politico_df[politico_df[\"Text lenght\"]>100]\n",
    "    return politico_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68aac75",
   "metadata": {},
   "source": [
    "## Washington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fe6454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links for all articles\n",
    "def links_wapo():\n",
    "    index = 0\n",
    "    dict_wapost = {} \n",
    "    url = \"https://www.washingtonpost.com/latest-headlines/\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    sections = soup.find_all(\"a\", {\"data-pb-local-content-field\":\"web_headline\"})\n",
    "    for i in sections:\n",
    "        link = i[\"href\"]\n",
    "        if \"podcasts\" not in link:\n",
    "            dict_wapost[index] = link\n",
    "            index +=1\n",
    "    return dict_wapost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e006d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the features for one article\n",
    "def attributes_wapo(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    # definitions\n",
    "    text = []\n",
    "    new_dict= {}\n",
    "    text_count = {}\n",
    "    \n",
    "    r = requests.get(article)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    content = soup.find_all(\"p\", {\"data-testid\":\"drop-cap-letter\"})\n",
    "    \n",
    "    # text and lenght\n",
    "    for i in content[:-1]:\n",
    "        y = i.get_text().strip()\n",
    "        text.append(y)\n",
    "    text =\"\".join(text)\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"span\", {\"data-qa\":\"headline-text\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "        \n",
    "    try:\n",
    "        date = soup.find(\"span\", {\"data-testid\":\"display-date\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            date = soup.find(\"span\", {\"data-testid\": \"updated-date\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            date = None\n",
    "            \n",
    "    try:\n",
    "        x = soup.find_all(\"a\", {\"data-qa\":\"author-name\"})\n",
    "        author = []\n",
    "        for i in x:\n",
    "            y = i.get_text().strip()\n",
    "            author.append(y)\n",
    "        author = \", \".join(author)\n",
    "    except AttributeError:\n",
    "        author = None\n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[0][\"srcset\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "        \n",
    "    category = article.split(\".com\")[1].split(\"/\")[1]\n",
    "\n",
    "    new_dict[0]={\"Source\":\"Washington Post\",\"Title\": title, \"Date\" : date,\"Author\" : author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : article, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4818330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "\n",
    "def parse_date(x):\n",
    "    try:\n",
    "        return parser.parse(x)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8732d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wapo_df():\n",
    "    dict_wapost = links_wapo()\n",
    "    dataframes_wapo = []\n",
    "    for url in dict_wapost.keys():\n",
    "        df = attributes_wapo(dict_wapost[url])\n",
    "        dataframes_wapo.append(df)\n",
    "\n",
    "    wapo_df = pd.concat(dataframes_wapo, ignore_index=True)\n",
    "    wapo_df[\"Date\"] = wapo_df[\"Date\"].apply(lambda x: None if x is None or \"ago\" in x else x)    \n",
    "    wapo_df[\"Date\"] = wapo_df[\"Date\"].apply(parse_date)\n",
    "    wapo_df[\"Date\"].dropna(inplace=True)\n",
    "    wapo_df[\"Date\"] = wapo_df[\"Date\"].astype(str)\n",
    "    wapo_df[\"Date\"] = wapo_df[\"Date\"].apply(lambda x : x.split(\".\")[0] if pd.notnull(x) else None)\n",
    "    wapo_df = wapo_df[wapo_df[\"Text lenght\"]>100]\n",
    "    return wapo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56430385",
   "metadata": {},
   "source": [
    "## New York Times | Do again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e1d0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links for all articles\n",
    "def links_nytimes():\n",
    "    index = 0\n",
    "    dict_nytimes = {} \n",
    "    url = \"https://www.nytimes.com/search?dropmab=false&query=news&sort=newest&types=article\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    sections = soup.find_all(\"div\", {\"class\":\"css-e1lvw9\"})\n",
    "    for i in sections:\n",
    "        link = f\"https://www.nytimes.com/{i.find('a')['href']}\"\n",
    "        link = link.split(\"?\")[0]\n",
    "        if \"podcast\" not in link:\n",
    "            dict_nytimes[index] = link\n",
    "            index +=1\n",
    "    return dict_nytimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7afd9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the features for one article\n",
    "def attributes_nytimes(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    # definitions\n",
    "    text = []\n",
    "    new_dict= {}\n",
    "    text_count = {}\n",
    "    \n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(article)\n",
    "    time.sleep(2)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    driver.quit()\n",
    "    # text and lenght\n",
    "    content = soup.find_all(\"p\")\n",
    "    for i in content[4:-3]:\n",
    "        y = i.get_text().strip()\n",
    "        text.append(y)\n",
    "    text =\"\".join(text)\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"h1\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "        \n",
    "    try:\n",
    "        date = soup.find(\"time\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "            \n",
    "    try:\n",
    "        x = soup.find_all(\"span\", {\"class\":\"css-1baulvz\"})\n",
    "        author = []\n",
    "        for i in x:\n",
    "            y = i.get_text().strip()\n",
    "            author.append(y)\n",
    "        author = \", \".join(author)\n",
    "    except AttributeError:\n",
    "        author = None\n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[0][\"srcset\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "        \n",
    "    category = article.split(\"2023\")[1].split(\"/\")[3]\n",
    "\n",
    "    new_dict[0]={\"Source\":\"New York Times\",\"Title\": title, \"Date\" : date,\"Author\" : author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : article, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09bb3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nytimes_df():\n",
    "    dataframes_nytimes = []\n",
    "    dict_nytimes = links_nytimes()\n",
    "    for url in dict_nytimes.keys():\n",
    "        df = attributes_nytimes(dict_nytimes[url])\n",
    "        dataframes_nytimes.append(df)\n",
    "    nytimes_df = pd.concat(dataframes_nytimes, ignore_index=True)\n",
    "    nytimes_df[\"Date\"] = nytimes_df[\"Date\"].apply(clean_html_tags)\n",
    "    nytimes_df[\"Date\"] = [remove_words(x, [\"Updated\", \"hours\", \"ago\"]) for x in nytimes_df[\"Date\"]]\n",
    "    nytimes_df[\"Date\"] = nytimes_df[\"Date\"].apply(lambda x: f\"{x.split('ET')[0]}ET\" if \"ET\" in x else x)\n",
    "    nytimes_df[\"Date\"] = nytimes_df[\"Date\"].apply(parse_date)\n",
    "    nytimes_df[\"Date\"].dropna(inplace=True)\n",
    "    nytimes_df[\"Date\"] = nytimes_df[\"Date\"].astype(str)\n",
    "    nytimes_df[\"Date\"] = nytimes_df[\"Date\"].apply(lambda x : x.split(\".\")[0] if pd.notnull(x) else None)\n",
    "    nytimes_df = nytimes_df[nytimes_df[\"Text lenght\"]>100]\n",
    "    return nytimes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd01ba",
   "metadata": {},
   "source": [
    "## MSNBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df64dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links for all articles\n",
    "def links_msnbc():\n",
    "    index = 0\n",
    "    dict_msnbc = {} \n",
    "    url = \"https://www.msnbc.com/\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    section = soup.find(\"div\", {\"class\":\"rail__container styles_rightRail__S621d layout-rightRail undefined layout-grid-container\"})\n",
    "    articles = section.find_all(\"h3\")\n",
    "    for i in articles:\n",
    "        link = i.find(\"a\")[\"href\"]\n",
    "        dict_msnbc[index] = link\n",
    "        index +=1\n",
    "        if index >= 11:\n",
    "            break\n",
    "    return dict_msnbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cc658c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the features for one article\n",
    "def attributes_msnbc(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    # definitions\n",
    "    text = []\n",
    "    new_dict= {}\n",
    "    \n",
    "    r = requests.get(article)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    \n",
    "    \n",
    "    # text and lenght\n",
    "    sec = soup.find_all(\"div\", {\"class\":\"article-body__content\"})\n",
    "    for i in sec:\n",
    "        x = i.find_all(\"p\")\n",
    "        for p in x:\n",
    "            y = p.get_text().strip()\n",
    "            text.append(y)\n",
    "    text = \" \".join(text)\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"h1\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "        \n",
    "    try:\n",
    "        date = soup.find(\"time\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "            \n",
    "    try:\n",
    "         author = soup.find(\"span\", {\"class\":\"byline-name\"}).get_text().strip().split(\",\")[0].split(\"By\")[0]\n",
    "    except AttributeError:\n",
    "        author = None\n",
    "        \n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[11][\"src\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "        \n",
    "    category = article.split(\".com\")[1].split(\"/\")[1]\n",
    "\n",
    "    new_dict[0]={\"Source\":\"MSNBC\",\"Title\": title, \"Date\" : date,\"Author\" : author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : article, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "227c4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msnbc_df():\n",
    "    dict_msnbc = links_msnbc()\n",
    "    dataframes_msnbc = []\n",
    "    for url in dict_msnbc.keys():\n",
    "        df = attributes_msnbc(dict_msnbc[url])\n",
    "        dataframes_msnbc.append(df)\n",
    "\n",
    "    msnbc_df = pd.concat(dataframes_msnbc, ignore_index=True)\n",
    "    msnbc_df[\"Date\"] = msnbc_df[\"Date\"].apply(lambda x : x.split(\"/\")[0])\n",
    "    msnbc_df[\"Date\"] = msnbc_df[\"Date\"].apply(lambda x : parser.parse(x) if pd.notnull(x) else None)\n",
    "    msnbc_df[\"Date\"].dropna(inplace=True)\n",
    "    msnbc_df[\"Date\"] = msnbc_df[\"Date\"].astype(str)\n",
    "    msnbc_df[\"Date\"] = msnbc_df[\"Date\"].apply(lambda x : x.split(\"+\")[0] if pd.notnull(x) else None)\n",
    "    msnbc_df = msnbc_df[msnbc_df[\"Text lenght\"]>100]\n",
    "    return msnbc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ca4a0",
   "metadata": {},
   "source": [
    "## Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "222595a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links for all articles\n",
    "def links_reuters():\n",
    "    index = 0\n",
    "    dict_reuters = {} \n",
    "    url = \"https://www.reuters.com/site-search/?query=news&date=past_24_hours&offset=0\"\n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    sections = soup.find_all(\"li\", {\"class\":\"search-results__item__2oqiX\"})\n",
    "    for i in range(len(sections)):\n",
    "        link = sections[i].find(\"a\")[\"href\"]\n",
    "        link = f\"https://www.reuters.com{link}\"\n",
    "        dict_reuters[index] = link\n",
    "        index +=1\n",
    "    driver.quit()\n",
    "    return dict_reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4eaca39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the features for one article\n",
    "def attributes_reuters(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    # definitions\n",
    "    new_dict= {}\n",
    "    author_list = []\n",
    "    \n",
    "    r = requests.get(article)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    \n",
    "    \n",
    "    # text and lenght\n",
    "    info = soup.find(\"div\", {\"class\":\"article-body__content__17Yit\"})\n",
    "    try:\n",
    "        content = info.find_all(\"p\")\n",
    "    except AttributeError:\n",
    "        content = soup.find_all(\"p\")\n",
    "\n",
    "    text = []\n",
    "    for i in content[:-4]:\n",
    "        y = i.get_text().strip()\n",
    "        text.append(y)\n",
    "    text = \"\".join(text)\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    try:\n",
    "        title = soup.find(\"h1\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "        \n",
    "    try:\n",
    "        date = soup.find(\"time\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "            \n",
    "    try:\n",
    "        info = soup.find(\"div\", {\"class\":\"info-content__author-date__1Epi_\"})\n",
    "        authors = info.find_all(\"a\")\n",
    "        for a in authors:\n",
    "            y = a.get_text()\n",
    "            author_list.append(y)\n",
    "        author = \", \".join(author_list)\n",
    "    except AttributeError:\n",
    "        author = None\n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[0][\"src\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "        \n",
    "    category = article.split(\".com\")[1].split(\"/\")[1]\n",
    "\n",
    "    new_dict[0]={\"Source\":\"Reuters\",\"Title\": title, \"Date\" : date,\"Author\" : author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : article, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f95d492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuters_df():\n",
    "    dict_reuters = links_reuters()\n",
    "    dataframes_reuters = []\n",
    "    for url in dict_reuters.keys():\n",
    "        df = attributes_reuters(dict_reuters[url])\n",
    "        dataframes_reuters.append(df)\n",
    "\n",
    "    reuters_df = pd.concat(dataframes_reuters, ignore_index=True)\n",
    "    reuters_df[\"Date\"] = reuters_df[\"Date\"].apply(lambda x : x.split(\"Updated\")[0] if pd.notnull(x) else None)\n",
    "    reuters_df[\"Date\"] = reuters_df[\"Date\"].apply(lambda x : \" \".join(x.split(\"2023\")) if pd.notnull(x) else None)\n",
    "    reuters_df[\"Date\"] = reuters_df[\"Date\"].apply(lambda x : parser.parse(x, fuzzy=True) if pd.notnull(x) else None)\n",
    "    reuters_df[\"Date\"].dropna(inplace=True)\n",
    "    reuters_df[\"Date\"] = reuters_df[\"Date\"].astype(str)\n",
    "    reuters_df[\"Date\"] = reuters_df[\"Date\"].apply(lambda x : x.split(\"+\")[0] if pd.notnull(x) else None)\n",
    "    reuters_df = reuters_df[reuters_df[\"Text lenght\"]>100]\n",
    "    return reuters_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc68eb",
   "metadata": {},
   "source": [
    "## USA TODAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2117ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links for all articles\n",
    "def links_usatoday():\n",
    "    index = 0\n",
    "    dict_usatoday = {} \n",
    "    url = \"https://eu.usatoday.com/\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    section = soup.find(\"div\", {\"class\":\"content-well\"})\n",
    "    links = section.find_all(\"a\")\n",
    "    for i in links:\n",
    "        link = i[\"href\"]\n",
    "        link = f\"https://eu.usatoday.com{link}\"\n",
    "        dict_usatoday[index] = link\n",
    "        index +=1\n",
    "    return dict_usatoday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b06852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the features for one article\n",
    "def attributes_usatoday(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    # definitions\n",
    "    text = []\n",
    "    new_dict= {}\n",
    "    \n",
    "    r = requests.get(article)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    \n",
    "    \n",
    "    # text and lenght\n",
    "    content = soup.find_all(\"p\")\n",
    "    for i in content:\n",
    "        y = i.get_text().strip()\n",
    "        text.append(y)\n",
    "    text = \" \".join(text)\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"h1\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "        \n",
    "    try:\n",
    "        date = soup.find(\"lit-timestamp\")[\"publishdate\"]\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "            \n",
    "    try:\n",
    "        authors = soup.find(\"span\", {\"class\":\"author\"})\n",
    "        authors = authors.find_all(\"a\")\n",
    "        author_list = []\n",
    "        for a in authors:\n",
    "            y = a.get_text().strip()\n",
    "            author_list.append(y)\n",
    "        author = \", \".join(author_list)\n",
    "    except AttributeError:\n",
    "        author = None\n",
    "        \n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[2][\"src\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "        \n",
    "    c = article.split(\"story\")[1]\n",
    "    if \"news\" in c:\n",
    "        category = c.split(\"/\")[2]\n",
    "    else:\n",
    "        category = c.split(\"/\")[1]\n",
    "\n",
    "    new_dict[0]={\"Source\":\"USA TODAY\",\"Title\": title, \"Date\" : date,\"Author\" : author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : article, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0312262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usatoday_df():\n",
    "    dict_usatoday = links_usatoday()\n",
    "    dataframes_usatoday = []\n",
    "    for url in dict_usatoday.keys():\n",
    "        df = attributes_usatoday(dict_usatoday[url])\n",
    "        dataframes_usatoday.append(df)\n",
    "\n",
    "    usatoday_df = pd.concat(dataframes_usatoday, ignore_index=True)\n",
    "    usatoday_df[\"Date\"] = usatoday_df[\"Date\"].apply(lambda x : parser.parse(x) if pd.notnull(x) else None)\n",
    "    usatoday_df[\"Date\"].dropna(inplace=True)\n",
    "    usatoday_df[\"Date\"] = usatoday_df[\"Date\"].astype(str)\n",
    "    usatoday_df[\"Date\"] = usatoday_df[\"Date\"].apply(lambda x : x.split(\"+\")[0] if pd.notnull(x) else None)\n",
    "    usatoday_df = usatoday_df[usatoday_df[\"Text lenght\"]>100]\n",
    "    return usatoday_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6a79331",
   "metadata": {},
   "outputs": [],
   "source": [
    "    dict_usatoday = links_usatoday()\n",
    "    dataframes_usatoday = []\n",
    "    for url in dict_usatoday.keys():\n",
    "        df = attributes_usatoday(dict_usatoday[url])\n",
    "        dataframes_usatoday.append(df)\n",
    "\n",
    "    usatoday_df = pd.concat(dataframes_usatoday, ignore_index=True)\n",
    "    usatoday_df[\"Date\"] = usatoday_df[\"Date\"].apply(lambda x : parser.parse(x) if pd.notnull(x) else None)\n",
    "    usatoday_df[\"Date\"].dropna(inplace=True)\n",
    "    usatoday_df[\"Date\"] = usatoday_df[\"Date\"].astype(str)\n",
    "    usatoday_df[\"Date\"] = usatoday_df[\"Date\"].apply(lambda x : x.split(\"+\")[0] if pd.notnull(x) else None)\n",
    "    usatoday_df = usatoday_df[usatoday_df[\"Text lenght\"]>100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57990c80",
   "metadata": {},
   "source": [
    "## NPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b069ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_npr():\n",
    "    index = 0\n",
    "    dict_npr = {} \n",
    "    url = \"https://www.npr.org/sections/news/\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    sections = soup.find_all(\"h2\", {\"class\":\"title\"})\n",
    "    for i in sections:\n",
    "        link = i.find(\"a\")[\"href\"]\n",
    "        dict_npr[index] = link\n",
    "        index += 1\n",
    "    return dict_npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65f09d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the features for one article\n",
    "def attributes_npr(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    # definitions\n",
    "    text = []\n",
    "    new_dict= {}\n",
    "    \n",
    "    r = requests.get(article)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    \n",
    "    \n",
    "    # text and lenght\n",
    "    content = soup.find_all(\"p\")\n",
    "    for i in content[4:-2]:\n",
    "        y = i.get_text().strip()\n",
    "        text.append(y)\n",
    "    text = \" \".join(text)\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"h1\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "        \n",
    "    try:\n",
    "        date = soup.find(\"span\", {\"class\":\"date\"}).get_text().strip()\n",
    "        date = f\"{date} {soup.find('span', {'class':'time'}).get_text().strip()}\"\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "            \n",
    "    try:\n",
    "        author = soup.find(\"p\", {\"class\":\"byline__name byline__name--block\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        author = None\n",
    "    try:\n",
    "        category = soup.find(\"a\", {\"class\":\"tag tag--story\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        category = None\n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[15][\"src\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "    \n",
    "    new_dict[0]={\"Source\":\"NPR\",\"Title\": title, \"Date\" : date,\"Author\" : author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : article, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db7931e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npr_df():\n",
    "    dict_npr = links_npr()\n",
    "    dataframes_npr = []\n",
    "    for url in dict_npr.keys():\n",
    "        df = attributes_npr(dict_npr[url])\n",
    "        dataframes_npr.append(df)\n",
    "\n",
    "    npr_df = pd.concat(dataframes_npr, ignore_index=True)\n",
    "    npr_df[\"Date\"] = npr_df[\"Date\"].apply(lambda x : parser.parse(x, fuzzy=True) if pd.notnull(x) else None)\n",
    "    npr_df[\"Date\"].dropna(inplace=True)\n",
    "    npr_df[\"Date\"] = npr_df[\"Date\"].astype(str)\n",
    "    npr_df[\"Date\"] = npr_df[\"Date\"].apply(lambda x : x.split(\".\")[0] if pd.notnull(x) else None)\n",
    "    npr_df = npr_df[npr_df[\"Text lenght\"]>100]\n",
    "    return npr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ecf0f",
   "metadata": {},
   "source": [
    "## Chicago Tribune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f22e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all links\n",
    "def links_ct():\n",
    "    index = 0\n",
    "    dict_ct = {} \n",
    "    url = \"https://www.chicagotribune.com/news/breaking/\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    sections = soup.find_all(\"article\", {\"class\":\"container-fluid row flex_row\"})\n",
    "    for i in sections:\n",
    "        link = i.find(\"a\")[\"href\"]\n",
    "        link = f\"https://www.chicagotribune.com{link}\"\n",
    "        dict_ct[index] = link\n",
    "        index +=1\n",
    "    return dict_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e37028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the features for one article\n",
    "def attributes_ct(article):\n",
    "    \"\"\"This function receives an url and returns a DF with different attributes\"\"\"\n",
    "    # definitions\n",
    "    new_dict= {}\n",
    "    \n",
    "    r = requests.get(article)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    \n",
    "    \n",
    "    # text and lenght\n",
    "    content = soup.find_all(\"p\", {\"class\":\"default__StyledText-sc-1wxyvyl-0 fxgoSg body-paragraph\"})\n",
    "    text = []\n",
    "    for i in content:\n",
    "        text.append(i.get_text().strip())\n",
    "    text = \"\".join(text)\n",
    "    text = clean_html_tags(text)\n",
    "    lenght = len(text)\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"h1\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "        \n",
    "    try:\n",
    "        date = soup.find(\"time\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "            \n",
    "    try:\n",
    "        x = soup.find(\"div\", {\"class\":\"article_byline\"})\n",
    "        x = x.find_all(\"a\")\n",
    "        y = []\n",
    "        for i in x:\n",
    "            y.append(i.get_text().strip())\n",
    "        author = \", \".join(y)\n",
    "    except AttributeError:\n",
    "        author = None\n",
    "    \n",
    "    c = article.split(\".com\")[1]\n",
    "    if \"news\" in c:\n",
    "        category = c.split(\"/\")[2]\n",
    "    else:\n",
    "        category = c.split(\"/\")[1]\n",
    "    try:\n",
    "        imageURL = soup.find_all(\"img\")[3][\"src\"]\n",
    "    except Exception:\n",
    "        imageURL= None\n",
    "    \n",
    "    new_dict[0]={\"Source\":\"Chicago Tribune\",\"Title\": title, \"Date\" : date,\"Author\" : author, \"Category\": category, \"Text\" : text , \"Text lenght\" : lenght, \"URL\" : article, \"imageURL\":imageURL}\n",
    "    df = pd.DataFrame.from_dict(new_dict, orient=\"index\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd93c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct_df():\n",
    "    dict_ct = links_ct()\n",
    "    dataframes_ct = []\n",
    "    for url in dict_ct.keys():\n",
    "        df = attributes_ct(dict_ct[url])\n",
    "        dataframes_ct.append(df)\n",
    "\n",
    "    ct_df = pd.concat(dataframes_ct, ignore_index=True)\n",
    "    ct_df[\"Date\"] = ct_df[\"Date\"].apply(lambda x : parser.parse(x) if pd.notnull(x) else None)\n",
    "    ct_df[\"Date\"].dropna(inplace=True)\n",
    "    ct_df[\"Date\"] = ct_df[\"Date\"].astype(str)\n",
    "    ct_df[\"Date\"] = ct_df[\"Date\"].apply(lambda x : x.split(\".\")[0] if pd.notnull(x) else None)\n",
    "    ct_df = ct_df[ct_df[\"Text lenght\"]>100]\n",
    "    return ct_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5c828",
   "metadata": {},
   "source": [
    "## Whole DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "017a0074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\Jacob\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\Jacob\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\Jacob\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\Jacob\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\Jacob\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "final_df1 = pd.concat([cnn_df(), fox_df(), abc_df(dict_abc), politico_df(), wapo_df(), nytimes_df(), msnbc_df()], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5375ebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "final_df2 = pd.concat([usatoday_df, npr_df(), ct_df()], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10d5fd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text lenght</th>\n",
       "      <th>URL</th>\n",
       "      <th>imageURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Details emerge about UNLV gunman who killed 3 ...</td>\n",
       "      <td>2023-12-08 03:06:00</td>\n",
       "      <td>Elizabeth Wolfe</td>\n",
       "      <td>us</td>\n",
       "      <td>Investigators searching for the motive of Anth...</td>\n",
       "      <td>3999</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/us/university-o...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Azerbaijan and Armenia agree to prisoner swap ...</td>\n",
       "      <td>2023-12-08 02:44:00</td>\n",
       "      <td>Angela Dewan</td>\n",
       "      <td>europe</td>\n",
       "      <td>Azerbaijan and Armenia have agreed to a prison...</td>\n",
       "      <td>3408</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/europe/azerbaij...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Italy quits Belt and Roadplan as Europe rethi...</td>\n",
       "      <td>2023-12-08 02:38:00</td>\n",
       "      <td>Simone McCarthy</td>\n",
       "      <td>china</td>\n",
       "      <td>Italy, the only G7 country to join Chinas fla...</td>\n",
       "      <td>4743</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/china/italy-bel...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>How the impasse over Ukraine aid could have cr...</td>\n",
       "      <td>2023-12-08 00:01:00</td>\n",
       "      <td>Stephen Collinson</td>\n",
       "      <td>politics</td>\n",
       "      <td>Americas paralyzing political estrangement ma...</td>\n",
       "      <td>10601</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/politics/congre...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Laws need to change: Stella McCartney calls ...</td>\n",
       "      <td>2023-12-07 22:58:00</td>\n",
       "      <td>Christy Choi</td>\n",
       "      <td>style</td>\n",
       "      <td>Stella McCartney has called on world leaders t...</td>\n",
       "      <td>4517</td>\n",
       "      <td>https://www.cnn.com/style/stella-mccartney-lea...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>Kennedy Expressway lanes to reopen, as first y...</td>\n",
       "      <td>2023-12-06 17:15:00</td>\n",
       "      <td>Sarah Freishtat</td>\n",
       "      <td>business</td>\n",
       "      <td>Kennedy Expressway drivers heading toward down...</td>\n",
       "      <td>2971</td>\n",
       "      <td>https://www.chicagotribune.com/business/ct-biz...</td>\n",
       "      <td>https://www.chicagotribune.com/resizer/5cAZOcy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>Advisory council created to ensure diversity i...</td>\n",
       "      <td>2023-12-06 16:57:00</td>\n",
       "      <td>Rick Pearson</td>\n",
       "      <td>politics</td>\n",
       "      <td>The host committee for next years Democratic ...</td>\n",
       "      <td>1896</td>\n",
       "      <td>https://www.chicagotribune.com/politics/ct-dnc...</td>\n",
       "      <td>https://www.chicagotribune.com/resizer/Zzsk8IB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>McDonalds will open first CosMcs spinoff in ...</td>\n",
       "      <td>2023-12-06 16:16:00</td>\n",
       "      <td>Talia Soglin</td>\n",
       "      <td>business</td>\n",
       "      <td>The week after photos of its new spinoff CosMc...</td>\n",
       "      <td>3937</td>\n",
       "      <td>https://www.chicagotribune.com/business/ct-biz...</td>\n",
       "      <td>https://www.chicagotribune.com/resizer/wyE3Veq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>Dolton trustees at odds with Mayor Tiffany Hen...</td>\n",
       "      <td>2023-12-06 15:28:00</td>\n",
       "      <td>Mike Nolan</td>\n",
       "      <td>suburbs</td>\n",
       "      <td>A day after walking out on a Village Board mee...</td>\n",
       "      <td>3656</td>\n",
       "      <td>https://www.chicagotribune.com/suburbs/daily-s...</td>\n",
       "      <td>https://www.chicagotribune.com/resizer/c1NKmKy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>Elgin cop charged with five counts of child po...</td>\n",
       "      <td>2023-12-06 14:56:00</td>\n",
       "      <td></td>\n",
       "      <td>elgin-courier-news</td>\n",
       "      <td>An Elgin police officer has been arrested on f...</td>\n",
       "      <td>2132</td>\n",
       "      <td>https://www.chicagotribune.com/suburbs/elgin-c...</td>\n",
       "      <td>https://www.chicagotribune.com/resizer/jHBP6Ay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Source                                              Title  \\\n",
       "0                CNN  Details emerge about UNLV gunman who killed 3 ...   \n",
       "1                CNN  Azerbaijan and Armenia agree to prisoner swap ...   \n",
       "2                CNN  Italy quits Belt and Roadplan as Europe rethi...   \n",
       "3                CNN  How the impasse over Ukraine aid could have cr...   \n",
       "4                CNN  Laws need to change: Stella McCartney calls ...   \n",
       "..               ...                                                ...   \n",
       "440  Chicago Tribune  Kennedy Expressway lanes to reopen, as first y...   \n",
       "441  Chicago Tribune  Advisory council created to ensure diversity i...   \n",
       "442  Chicago Tribune  McDonalds will open first CosMcs spinoff in ...   \n",
       "443  Chicago Tribune  Dolton trustees at odds with Mayor Tiffany Hen...   \n",
       "444  Chicago Tribune  Elgin cop charged with five counts of child po...   \n",
       "\n",
       "                    Date             Author            Category  \\\n",
       "0    2023-12-08 03:06:00    Elizabeth Wolfe                  us   \n",
       "1    2023-12-08 02:44:00       Angela Dewan              europe   \n",
       "2    2023-12-08 02:38:00    Simone McCarthy               china   \n",
       "3    2023-12-08 00:01:00  Stephen Collinson            politics   \n",
       "4    2023-12-07 22:58:00       Christy Choi               style   \n",
       "..                   ...                ...                 ...   \n",
       "440  2023-12-06 17:15:00    Sarah Freishtat            business   \n",
       "441  2023-12-06 16:57:00       Rick Pearson            politics   \n",
       "442  2023-12-06 16:16:00       Talia Soglin            business   \n",
       "443  2023-12-06 15:28:00         Mike Nolan             suburbs   \n",
       "444  2023-12-06 14:56:00                     elgin-courier-news   \n",
       "\n",
       "                                                  Text  Text lenght  \\\n",
       "0    Investigators searching for the motive of Anth...         3999   \n",
       "1    Azerbaijan and Armenia have agreed to a prison...         3408   \n",
       "2    Italy, the only G7 country to join Chinas fla...         4743   \n",
       "3    Americas paralyzing political estrangement ma...        10601   \n",
       "4    Stella McCartney has called on world leaders t...         4517   \n",
       "..                                                 ...          ...   \n",
       "440  Kennedy Expressway drivers heading toward down...         2971   \n",
       "441  The host committee for next years Democratic ...         1896   \n",
       "442  The week after photos of its new spinoff CosMc...         3937   \n",
       "443  A day after walking out on a Village Board mee...         3656   \n",
       "444  An Elgin police officer has been arrested on f...         2132   \n",
       "\n",
       "                                                   URL  \\\n",
       "0    https://www.cnn.com/2023/12/08/us/university-o...   \n",
       "1    https://www.cnn.com/2023/12/08/europe/azerbaij...   \n",
       "2    https://www.cnn.com/2023/12/08/china/italy-bel...   \n",
       "3    https://www.cnn.com/2023/12/08/politics/congre...   \n",
       "4    https://www.cnn.com/style/stella-mccartney-lea...   \n",
       "..                                                 ...   \n",
       "440  https://www.chicagotribune.com/business/ct-biz...   \n",
       "441  https://www.chicagotribune.com/politics/ct-dnc...   \n",
       "442  https://www.chicagotribune.com/business/ct-biz...   \n",
       "443  https://www.chicagotribune.com/suburbs/daily-s...   \n",
       "444  https://www.chicagotribune.com/suburbs/elgin-c...   \n",
       "\n",
       "                                              imageURL  \n",
       "0    https://media.cnn.com/api/v1/images/stellar/pr...  \n",
       "1    https://media.cnn.com/api/v1/images/stellar/pr...  \n",
       "2    https://media.cnn.com/api/v1/images/stellar/pr...  \n",
       "3    https://media.cnn.com/api/v1/images/stellar/pr...  \n",
       "4    https://media.cnn.com/api/v1/images/stellar/pr...  \n",
       "..                                                 ...  \n",
       "440  https://www.chicagotribune.com/resizer/5cAZOcy...  \n",
       "441  https://www.chicagotribune.com/resizer/Zzsk8IB...  \n",
       "442  https://www.chicagotribune.com/resizer/wyE3Veq...  \n",
       "443  https://www.chicagotribune.com/resizer/c1NKmKy...  \n",
       "444  https://www.chicagotribune.com/resizer/jHBP6Ay...  \n",
       "\n",
       "[445 rows x 9 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.concat([final_df1, final_df2], ignore_index=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd871345",
   "metadata": {},
   "source": [
    "## Data Cleaning and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bba7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5df9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"] = df[\"Category\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "235c3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=\"URL\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03d0c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_25884\\48230710.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=\"Title\", inplace=True)\n",
    "df[\"Title\"] = df[\"Title\"].apply(clean_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ece5f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_categories(x):\n",
    "    category_mapping = {\n",
    "        'International News': ['world',\"ukraine\",'israel-hamas war','israel','gaza', 'israel-gaza war' 'australia','india', 'china', 'americas', 'middleeast','international','israel hamas War', 'africa', 'asia', 'europe'],\n",
    "        'Politics': ['nation', \"election\", \"immigration\", 'new york', 'Congress','us', 'politics'],\n",
    "        'Business and Economy': ['economy','investing', 'business', 'markets', 'money'],\n",
    "        'Entertainment and Lifestyle': [\"lifestyle\",\"travel\", 'entertainment', 'cars', 'culture', 'food', 'style', 'tech','advice', 'success', 'books', 'cruise ship', 'wellness', 'family', 'life expectancy'],\n",
    "        'Climate and Environment': ['climate','energy',\"environment\", 'climate-environment', 'climate-solutions'],\n",
    "        'Health | Science | Technology': ['health', 'science', 'technology', 'artificial intelligence'],\n",
    "        'Sports': ['sport', 'sports'],\n",
    "        'Law and Justice': ['national-security','legal', 'criminajustice', \"retail theft\", \"financial crimes\", \"crime\",  ],\n",
    "        'del' : [\"weather\" ] \n",
    "    }\n",
    "\n",
    "    categorized_result = {category: [] for category in category_mapping.keys()}\n",
    "    matched_category = None\n",
    "    for main_category, sub_categories in category_mapping.items():\n",
    "        if any(sub_category in x.lower() for sub_category in sub_categories):\n",
    "            matched_category = main_category\n",
    "            break\n",
    "    \n",
    "    return matched_category\n",
    "\n",
    "df[\"cat_label\"] = df[\"Category\"].apply(categorize_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "361c634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"cat_label\"]!=\"del\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57c15a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2023-12-08 03:06:00\n",
       "1     2023-12-08 02:44:00\n",
       "2     2023-12-08 02:38:00\n",
       "3     2023-12-08 00:01:00\n",
       "4     2023-12-07 22:58:00\n",
       "              ...        \n",
       "440   2023-12-06 17:15:00\n",
       "441   2023-12-06 16:57:00\n",
       "442   2023-12-06 16:16:00\n",
       "443   2023-12-06 15:28:00\n",
       "444   2023-12-06 14:56:00\n",
       "Name: Date, Length: 332, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df[\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf74a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_replacements = {\n",
    "    \"CNN\":\"https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx3PvP_-LsgjQHaE7&pid=Api&P=0&h=180\",\n",
    "    \"Fox News\": \"https://tse1.mm.bing.net/th?id=OIP.cgyUI9sP8zj7Zhf5tpOI2wHaHa&pid=Api&P=0&h=180\",\n",
    "    \"ABC\" : \"https://tse1.mm.bing.net/th?id=OIP.KAIEmdZnussbmh0xL1UqIAHaHa&pid=Api&P=0&h=180\",\n",
    "    \"Politico\": \"https://tse1.mm.bing.net/th?id=OIP.GNggW5ET63WsrjcQpARO4QHaHa&pid=Api&P=0&h=180\",\n",
    "    \"Washington Post\" : \"https://tse3.explicit.bing.net/th?id=OIP.cysHEGJuAL_FgzGaxQpl1QHaHa&pid=Api&P=0&h=180\",\n",
    "    'New York Times': \"https://tse2.mm.bing.net/th?id=OIP.jVot1lhu9PmkBYqVP_y4QAHaGg&pid=Api&P=0&h=180\",\n",
    "    'MSNBC': \"https://tse4.mm.bing.net/th?id=OIP.0QnXrS2STCJkb2iD0VvN8AHaHa&pid=Api&P=0&h=180\",\n",
    "    'Reuters': \"https://tse1.mm.bing.net/th?id=OIP.jw0U2QGUAVScpEN8_Ik8UQHaHa&pid=Api&P=0&h=180\",\n",
    "    'USA TODAY': \"https://tse4.mm.bing.net/th?id=OIP.SY6y2UDBYYuKUPjSwFXbegHaHa&pid=Api&P=0&h=180\",\n",
    "    'NPR': \"https://tse4.mm.bing.net/th?id=OIP.VhogjnWRzUbgzzIcAE8DswHaHa&pid=Api&P=0&h=180\",\n",
    "    'Chicago Tribune': \"https://tse1.mm.bing.net/th?id=OIP.bdQ4hj2mS6umETrhkh7uTQAAAA&pid=Api&P=0&h=180\"\n",
    "}\n",
    "df[\"sourceURL\"] = df[\"Source\"].apply(lambda x: img_replacements[x])\n",
    "df.loc[df[\"imageURL\"].isnull(), \"imageURL\"] = df.loc[df[\"imageURL\"].isnull(), \"sourceURL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b766d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                         134\n",
       "Entertainment and Lifestyle       46\n",
       "Sports                            29\n",
       "International News                28\n",
       "Health | Science | Technology     10\n",
       "Business and Economy               6\n",
       "Climate and Environment            4\n",
       "Law and Justice                    1\n",
       "Name: cat_label, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cat_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80a6b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecd578cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3d41141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN                92\n",
       "ABC                58\n",
       "Fox News           44\n",
       "Washington Post    38\n",
       "Chicago Tribune    25\n",
       "NPR                22\n",
       "Politico           20\n",
       "MSNBC              11\n",
       "USA TODAY          10\n",
       "New York Times      6\n",
       "Name: Source, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284398ef",
   "metadata": {},
   "source": [
    "## Prep for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af86cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #Natural Language tool kit -- this pacakge is quite a mess. Was poorly design and the documentation is not great\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d8e62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_and_remove_punctuation(row):\n",
    "    tokens = word_tokenize(row['Text'])\n",
    "    return [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "df['tokenized'] = df.apply(tokenizer_and_remove_punctuation,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b85e3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word], lang='eng')[0][1][0].upper() # gets first letter of POS categorization\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7505a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer_with_pos(row):\n",
    "      return [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in row['tokenized']]\n",
    "\n",
    "df['lemmatized'] = df.apply(lemmatizer_with_pos,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "332756de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "\n",
    "def remove_sw(row):\n",
    "      return list(set(row['lemmatized']).difference(stopwords.words()))\n",
    "\n",
    "df['no_stopwords'] = df.apply(remove_sw,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cad948b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_blob(row):\n",
    "      return \" \".join(row['no_stopwords'])\n",
    "\n",
    "df['clean_blob'] = df.apply(re_blob,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea92f6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text lenght</th>\n",
       "      <th>URL</th>\n",
       "      <th>imageURL</th>\n",
       "      <th>cat_label</th>\n",
       "      <th>sourceURL</th>\n",
       "      <th>clean_blob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Details emerge about UNLV gunman who killed 3 ...</td>\n",
       "      <td>2023-12-08 03:06:00</td>\n",
       "      <td>Elizabeth Wolfe</td>\n",
       "      <td>us</td>\n",
       "      <td>Investigators searching for the motive of Anth...</td>\n",
       "      <td>3999</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/us/university-o...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>police country slain return relative patricia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Azerbaijan and Armenia agree to prisoner swap ...</td>\n",
       "      <td>2023-12-08 02:44:00</td>\n",
       "      <td>Angela Dewan</td>\n",
       "      <td>europe</td>\n",
       "      <td>Azerbaijan and Armenia have agreed to a prison...</td>\n",
       "      <td>3408</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/europe/azerbaij...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>International News</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>country european climate prosperous export enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Italy quits Belt and Road plan as Europe rethi...</td>\n",
       "      <td>2023-12-08 02:38:00</td>\n",
       "      <td>Simone McCarthy</td>\n",
       "      <td>china</td>\n",
       "      <td>Italy, the only G7 country to join Chinas fla...</td>\n",
       "      <td>4743</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/china/italy-bel...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>International News</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>country european ursula raise reason refer top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>How the impasse over Ukraine aid could have cr...</td>\n",
       "      <td>2023-12-08 00:01:00</td>\n",
       "      <td>Stephen Collinson</td>\n",
       "      <td>politics</td>\n",
       "      <td>Americas paralyzing political estrangement ma...</td>\n",
       "      <td>10601</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/politics/congre...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>country philosophical raise spring anna increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Laws need to change: Stella McCartney calls ...</td>\n",
       "      <td>2023-12-07 22:58:00</td>\n",
       "      <td>Christy Choi</td>\n",
       "      <td>style</td>\n",
       "      <td>Stella McCartney has called on world leaders t...</td>\n",
       "      <td>4517</td>\n",
       "      <td>https://www.cnn.com/style/stella-mccartney-lea...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>Entertainment and Lifestyle</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>climate showcased available enter combination ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source                                              Title  \\\n",
       "0    CNN  Details emerge about UNLV gunman who killed 3 ...   \n",
       "1    CNN  Azerbaijan and Armenia agree to prisoner swap ...   \n",
       "2    CNN  Italy quits Belt and Road plan as Europe rethi...   \n",
       "3    CNN  How the impasse over Ukraine aid could have cr...   \n",
       "4    CNN  Laws need to change: Stella McCartney calls ...   \n",
       "\n",
       "                 Date             Author  Category  \\\n",
       "0 2023-12-08 03:06:00    Elizabeth Wolfe        us   \n",
       "1 2023-12-08 02:44:00       Angela Dewan    europe   \n",
       "2 2023-12-08 02:38:00    Simone McCarthy     china   \n",
       "3 2023-12-08 00:01:00  Stephen Collinson  politics   \n",
       "4 2023-12-07 22:58:00       Christy Choi     style   \n",
       "\n",
       "                                                Text  Text lenght  \\\n",
       "0  Investigators searching for the motive of Anth...         3999   \n",
       "1  Azerbaijan and Armenia have agreed to a prison...         3408   \n",
       "2  Italy, the only G7 country to join Chinas fla...         4743   \n",
       "3  Americas paralyzing political estrangement ma...        10601   \n",
       "4  Stella McCartney has called on world leaders t...         4517   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.cnn.com/2023/12/08/us/university-o...   \n",
       "1  https://www.cnn.com/2023/12/08/europe/azerbaij...   \n",
       "2  https://www.cnn.com/2023/12/08/china/italy-bel...   \n",
       "3  https://www.cnn.com/2023/12/08/politics/congre...   \n",
       "4  https://www.cnn.com/style/stella-mccartney-lea...   \n",
       "\n",
       "                                            imageURL  \\\n",
       "0  https://media.cnn.com/api/v1/images/stellar/pr...   \n",
       "1  https://media.cnn.com/api/v1/images/stellar/pr...   \n",
       "2  https://media.cnn.com/api/v1/images/stellar/pr...   \n",
       "3  https://media.cnn.com/api/v1/images/stellar/pr...   \n",
       "4  https://media.cnn.com/api/v1/images/stellar/pr...   \n",
       "\n",
       "                     cat_label  \\\n",
       "0                     Politics   \n",
       "1           International News   \n",
       "2           International News   \n",
       "3                     Politics   \n",
       "4  Entertainment and Lifestyle   \n",
       "\n",
       "                                           sourceURL  \\\n",
       "0  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "1  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "2  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "3  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "4  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "\n",
       "                                          clean_blob  \n",
       "0  police country slain return relative patricia ...  \n",
       "1  country european climate prosperous export enc...  \n",
       "2  country european ursula raise reason refer top...  \n",
       "3  country philosophical raise spring anna increa...  \n",
       "4  climate showcased available enter combination ...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop([\"tokenized\", \"lemmatized\", \"no_stopwords\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1862627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                         132\n",
       "Entertainment and Lifestyle       46\n",
       "Sports                            29\n",
       "International News                28\n",
       "Health | Science | Technology     10\n",
       "Business and Economy               6\n",
       "Climate and Environment            4\n",
       "Law and Justice                    1\n",
       "Name: cat_label, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cat_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24cf688",
   "metadata": {},
   "source": [
    "## Category model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6fbad0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('bow_vect.pkl', 'rb') as file:\n",
    "    bow_vect = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a264cf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('cat_model.pkl', 'rb') as file:\n",
    "    cat_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73ab28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unlabeled = bow_vect.transform(df['clean_blob']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a89294cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pred_cat\"] = cat_model.predict(X_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62f23351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Source</th>\n",
       "      <th>Category</th>\n",
       "      <th>cat_label</th>\n",
       "      <th>pred_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Pennsylvania school board president sworn into...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>media</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Hunter indictment a 'nuclear bomb for the Bide...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>media</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Washington Post staffers launch 24-hour walkout</td>\n",
       "      <td>Politico</td>\n",
       "      <td>Labor</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Californias budget deficit swells to record $...</td>\n",
       "      <td>Politico</td>\n",
       "      <td>California</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Blinken explains his reaction to Biden calling...</td>\n",
       "      <td>Politico</td>\n",
       "      <td>foreign affairs</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>The Pain and the Trauma Lasts Longer Than a N...</td>\n",
       "      <td>Politico</td>\n",
       "      <td>q&amp;a</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>A China brawl looms for House Republicans</td>\n",
       "      <td>Politico</td>\n",
       "      <td>Finance &amp; Tax</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>McCarthys exit and the ripple effects back home</td>\n",
       "      <td>Politico</td>\n",
       "      <td>California</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Legal weed takes effect in Ohio as lawmakers s...</td>\n",
       "      <td>Politico</td>\n",
       "      <td>Cannabis</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>House Education committee to launch probe into...</td>\n",
       "      <td>Politico</td>\n",
       "      <td>Education</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Top lawmakers drop abortion limits from defens...</td>\n",
       "      <td>Politico</td>\n",
       "      <td>Defense</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Youngkin proposes child care, early education ...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>dc-md-va</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Pantone announces its 2024 color of the year</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>home</td>\n",
       "      <td>None</td>\n",
       "      <td>Entertainment and Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>The final humiliation of Kevin McCarthy</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Who won the fourth Republican presidential d...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>The GOP just conflated antisemitism with anti-...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Making Squid Game a reality show ruins the w...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>I had cancer as a kid. Trump threatening Obama...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Heres what Nikki Haley needs to do to beat Tr...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>I have bad news for the progressives currently...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>A new 2024 youth poll doesnt look promising f...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Pretending to care this much about football is...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Trump's second-term threats are the story of 2024</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>COP28's alarming conflict of interest</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>After Supreme Court's affirmative action rulin...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>education</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Are you single? Try the triangle method.</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>life</td>\n",
       "      <td>None</td>\n",
       "      <td>Entertainment and Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Flailing DeSantis campaign leans into anti-LGB...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>opinion</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>Musk vs. Zuckerberg Threads throwdown: Twitter...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>news</td>\n",
       "      <td>None</td>\n",
       "      <td>Entertainment and Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Former police chief turned yoga teacher senten...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Alan Hostetter</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>3 UNLV faculty members were killed in the Las ...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>mandalay bay</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Hunter Biden is indicted on federal tax charges</td>\n",
       "      <td>NPR</td>\n",
       "      <td>special counsel david weiss</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>House votes to censure Rep. Jamaal Bowman for ...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>censured</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Centenarian Pearl Harbor survivors return to h...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Pearl Harbor attack</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Emhoff says 3 college presidents showed a 'lac...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>antisemitism</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Pantone's color of the year is 'Peach Fuzz'</td>\n",
       "      <td>NPR</td>\n",
       "      <td>pantone color institute</td>\n",
       "      <td>None</td>\n",
       "      <td>Entertainment and Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Women Candidates and the Race for Big Money</td>\n",
       "      <td>NPR</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Entertainment and Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Juanita Castro, anti-communist sister of Cuban...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Advertiser backlash may pose mortal threat to ...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>advertisers</td>\n",
       "      <td>None</td>\n",
       "      <td>Entertainment and Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>This year's Hanukkah celebrations are tempered...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>7 in 10 U.S. adults consider themselves spiritual</td>\n",
       "      <td>NPR</td>\n",
       "      <td>spirituality</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>CosMc's lands in Illinois, as McDonald's tests...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>CosMc's</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>This African bird will lead you to honey, if y...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>animal-human relationships</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>White House proposes to 'march in' on patents ...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>drug prices</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Medicare open enrollment ends today. Ignoring ...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Medicare open enrollment</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Peru braces for protests after former Presiden...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Alberto Fujimori</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>How the Republican presidential candidates vie...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>voting stories</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Up First briefing: Key GOP debate takeaways; I...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Hinsdale Central girls basketball staffs abru...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>suburbs</td>\n",
       "      <td>None</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Caretaker gets 55-year sentence in stabbing de...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>breaking</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Off-duty officer driving SUV fatally strikes p...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>breaking</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Suspect in Romeoville killings threatened to ...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>breaking</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Early morning fire in Elmwood Park displaces a...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>suburbs</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Mokena family safe after house fire, but deput...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>suburbs</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Four connected to robbery crew charged in 3 ar...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>breaking</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Will County Judge David Carlson calls for spec...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>suburbs</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>Man dies in car accident overnight on Northwes...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>breaking</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Man fatally wounded overnight in West Loop nei...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>breaking</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Mother in critical condition, sons body pulle...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>breaking</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Dolton trustees at odds with Mayor Tiffany Hen...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>suburbs</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>Elgin cop charged with five counts of child po...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>elgin-courier-news</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title           Source  \\\n",
       "131  Pennsylvania school board president sworn into...         Fox News   \n",
       "134  Hunter indictment a 'nuclear bomb for the Bide...         Fox News   \n",
       "194    Washington Post staffers launch 24-hour walkout         Politico   \n",
       "196  Californias budget deficit swells to record $...         Politico   \n",
       "199  Blinken explains his reaction to Biden calling...         Politico   \n",
       "201  The Pain and the Trauma Lasts Longer Than a N...         Politico   \n",
       "202          A China brawl looms for House Republicans         Politico   \n",
       "208   McCarthys exit and the ripple effects back home         Politico   \n",
       "209  Legal weed takes effect in Ohio as lawmakers s...         Politico   \n",
       "210  House Education committee to launch probe into...         Politico   \n",
       "211  Top lawmakers drop abortion limits from defens...         Politico   \n",
       "219  Youngkin proposes child care, early education ...  Washington Post   \n",
       "240       Pantone announces its 2024 color of the year  Washington Post   \n",
       "258            The final humiliation of Kevin McCarthy            MSNBC   \n",
       "259  Who won the fourth Republican presidential d...            MSNBC   \n",
       "260  The GOP just conflated antisemitism with anti-...            MSNBC   \n",
       "261  Making Squid Game a reality show ruins the w...            MSNBC   \n",
       "262  I had cancer as a kid. Trump threatening Obama...            MSNBC   \n",
       "263  Heres what Nikki Haley needs to do to beat Tr...            MSNBC   \n",
       "264  I have bad news for the progressives currently...            MSNBC   \n",
       "265  A new 2024 youth poll doesnt look promising f...            MSNBC   \n",
       "266  Pretending to care this much about football is...            MSNBC   \n",
       "267  Trump's second-term threats are the story of 2024            MSNBC   \n",
       "268              COP28's alarming conflict of interest            MSNBC   \n",
       "269  After Supreme Court's affirmative action rulin...        USA TODAY   \n",
       "275           Are you single? Try the triangle method.        USA TODAY   \n",
       "276  Flailing DeSantis campaign leans into anti-LGB...        USA TODAY   \n",
       "278  Musk vs. Zuckerberg Threads throwdown: Twitter...        USA TODAY   \n",
       "279  Former police chief turned yoga teacher senten...              NPR   \n",
       "280  3 UNLV faculty members were killed in the Las ...              NPR   \n",
       "281    Hunter Biden is indicted on federal tax charges              NPR   \n",
       "282  House votes to censure Rep. Jamaal Bowman for ...              NPR   \n",
       "283  Centenarian Pearl Harbor survivors return to h...              NPR   \n",
       "284  Emhoff says 3 college presidents showed a 'lac...              NPR   \n",
       "285        Pantone's color of the year is 'Peach Fuzz'              NPR   \n",
       "286        Women Candidates and the Race for Big Money              NPR   \n",
       "287  Juanita Castro, anti-communist sister of Cuban...              NPR   \n",
       "288  Advertiser backlash may pose mortal threat to ...              NPR   \n",
       "289  This year's Hanukkah celebrations are tempered...              NPR   \n",
       "290  7 in 10 U.S. adults consider themselves spiritual              NPR   \n",
       "291  CosMc's lands in Illinois, as McDonald's tests...              NPR   \n",
       "292  This African bird will lead you to honey, if y...              NPR   \n",
       "294  White House proposes to 'march in' on patents ...              NPR   \n",
       "295  Medicare open enrollment ends today. Ignoring ...              NPR   \n",
       "296  Peru braces for protests after former Presiden...              NPR   \n",
       "297  How the Republican presidential candidates vie...              NPR   \n",
       "298  Up First briefing: Key GOP debate takeaways; I...              NPR   \n",
       "301  Hinsdale Central girls basketball staffs abru...  Chicago Tribune   \n",
       "304  Caretaker gets 55-year sentence in stabbing de...  Chicago Tribune   \n",
       "306  Off-duty officer driving SUV fatally strikes p...  Chicago Tribune   \n",
       "307  Suspect in Romeoville killings threatened to ...  Chicago Tribune   \n",
       "311  Early morning fire in Elmwood Park displaces a...  Chicago Tribune   \n",
       "312  Mokena family safe after house fire, but deput...  Chicago Tribune   \n",
       "313  Four connected to robbery crew charged in 3 ar...  Chicago Tribune   \n",
       "314  Will County Judge David Carlson calls for spec...  Chicago Tribune   \n",
       "315  Man dies in car accident overnight on Northwes...  Chicago Tribune   \n",
       "316  Man fatally wounded overnight in West Loop nei...  Chicago Tribune   \n",
       "317  Mother in critical condition, sons body pulle...  Chicago Tribune   \n",
       "324  Dolton trustees at odds with Mayor Tiffany Hen...  Chicago Tribune   \n",
       "325  Elgin cop charged with five counts of child po...  Chicago Tribune   \n",
       "\n",
       "                        Category cat_label                     pred_cat  \n",
       "131                        media      None                     Politics  \n",
       "134                        media      None                     Politics  \n",
       "194                        Labor      None                     Politics  \n",
       "196                   California      None                     Politics  \n",
       "199              foreign affairs      None                     Politics  \n",
       "201                          q&a      None                     Politics  \n",
       "202                Finance & Tax      None                     Politics  \n",
       "208                   California      None                     Politics  \n",
       "209                     Cannabis      None                     Politics  \n",
       "210                    Education      None                     Politics  \n",
       "211                      Defense      None                     Politics  \n",
       "219                     dc-md-va      None                     Politics  \n",
       "240                         home      None  Entertainment and Lifestyle  \n",
       "258                      opinion      None                     Politics  \n",
       "259                      opinion      None                     Politics  \n",
       "260                      opinion      None                     Politics  \n",
       "261                      opinion      None                     Politics  \n",
       "262                      opinion      None                     Politics  \n",
       "263                      opinion      None                     Politics  \n",
       "264                      opinion      None                     Politics  \n",
       "265                      opinion      None                     Politics  \n",
       "266                      opinion      None                     Politics  \n",
       "267                      opinion      None                     Politics  \n",
       "268                      opinion      None                     Politics  \n",
       "269                    education      None                     Politics  \n",
       "275                         life      None  Entertainment and Lifestyle  \n",
       "276                      opinion      None                     Politics  \n",
       "278                         news      None  Entertainment and Lifestyle  \n",
       "279               Alan Hostetter      None                     Politics  \n",
       "280                 mandalay bay      None                     Politics  \n",
       "281  special counsel david weiss      None                     Politics  \n",
       "282                     censured      None                     Politics  \n",
       "283          Pearl Harbor attack      None                     Politics  \n",
       "284                 antisemitism      None                     Politics  \n",
       "285      pantone color institute      None  Entertainment and Lifestyle  \n",
       "286                         None      None  Entertainment and Lifestyle  \n",
       "287                         Cuba      None                     Politics  \n",
       "288                  advertisers      None  Entertainment and Lifestyle  \n",
       "289                         None      None                     Politics  \n",
       "290                 spirituality      None                     Politics  \n",
       "291                      CosMc's      None                     Politics  \n",
       "292   animal-human relationships      None                     Politics  \n",
       "294                  drug prices      None                     Politics  \n",
       "295     Medicare open enrollment      None                     Politics  \n",
       "296             Alberto Fujimori      None                     Politics  \n",
       "297               voting stories      None                     Politics  \n",
       "298                         None      None                     Politics  \n",
       "301                      suburbs      None                       Sports  \n",
       "304                     breaking      None                     Politics  \n",
       "306                     breaking      None                     Politics  \n",
       "307                     breaking      None                     Politics  \n",
       "311                      suburbs      None                     Politics  \n",
       "312                      suburbs      None                     Politics  \n",
       "313                     breaking      None                     Politics  \n",
       "314                      suburbs      None                     Politics  \n",
       "315                     breaking      None                     Politics  \n",
       "316                     breaking      None                     Politics  \n",
       "317                     breaking      None                     Politics  \n",
       "324                      suburbs      None                     Politics  \n",
       "325           elgin-courier-news      None                     Politics  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = df[[\"Title\",\"Source\",\"Category\",\"cat_label\", \"pred_cat\"]]\n",
    "check[check[\"cat_label\"].isnull()].tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "266f15e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text lenght</th>\n",
       "      <th>URL</th>\n",
       "      <th>imageURL</th>\n",
       "      <th>cat_label</th>\n",
       "      <th>sourceURL</th>\n",
       "      <th>clean_blob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Details emerge about UNLV gunman who killed 3 ...</td>\n",
       "      <td>2023-12-08 03:06:00</td>\n",
       "      <td>Elizabeth Wolfe</td>\n",
       "      <td>Investigators searching for the motive of Anth...</td>\n",
       "      <td>3999</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/us/university-o...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>police country slain return relative patricia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Azerbaijan and Armenia agree to prisoner swap ...</td>\n",
       "      <td>2023-12-08 02:44:00</td>\n",
       "      <td>Angela Dewan</td>\n",
       "      <td>Azerbaijan and Armenia have agreed to a prison...</td>\n",
       "      <td>3408</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/europe/azerbaij...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>International News</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>country european climate prosperous export enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Italy quits Belt and Road plan as Europe rethi...</td>\n",
       "      <td>2023-12-08 02:38:00</td>\n",
       "      <td>Simone McCarthy</td>\n",
       "      <td>Italy, the only G7 country to join Chinas fla...</td>\n",
       "      <td>4743</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/china/italy-bel...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>International News</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>country european ursula raise reason refer top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>How the impasse over Ukraine aid could have cr...</td>\n",
       "      <td>2023-12-08 00:01:00</td>\n",
       "      <td>Stephen Collinson</td>\n",
       "      <td>Americas paralyzing political estrangement ma...</td>\n",
       "      <td>10601</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/politics/congre...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>country philosophical raise spring anna increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Laws need to change: Stella McCartney calls ...</td>\n",
       "      <td>2023-12-07 22:58:00</td>\n",
       "      <td>Christy Choi</td>\n",
       "      <td>Stella McCartney has called on world leaders t...</td>\n",
       "      <td>4517</td>\n",
       "      <td>https://www.cnn.com/style/stella-mccartney-lea...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>Entertainment and Lifestyle</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>climate showcased available enter combination ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source                                              Title  \\\n",
       "0    CNN  Details emerge about UNLV gunman who killed 3 ...   \n",
       "1    CNN  Azerbaijan and Armenia agree to prisoner swap ...   \n",
       "2    CNN  Italy quits Belt and Road plan as Europe rethi...   \n",
       "3    CNN  How the impasse over Ukraine aid could have cr...   \n",
       "4    CNN  Laws need to change: Stella McCartney calls ...   \n",
       "\n",
       "                 Date             Author  \\\n",
       "0 2023-12-08 03:06:00    Elizabeth Wolfe   \n",
       "1 2023-12-08 02:44:00       Angela Dewan   \n",
       "2 2023-12-08 02:38:00    Simone McCarthy   \n",
       "3 2023-12-08 00:01:00  Stephen Collinson   \n",
       "4 2023-12-07 22:58:00       Christy Choi   \n",
       "\n",
       "                                                Text  Text lenght  \\\n",
       "0  Investigators searching for the motive of Anth...         3999   \n",
       "1  Azerbaijan and Armenia have agreed to a prison...         3408   \n",
       "2  Italy, the only G7 country to join Chinas fla...         4743   \n",
       "3  Americas paralyzing political estrangement ma...        10601   \n",
       "4  Stella McCartney has called on world leaders t...         4517   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.cnn.com/2023/12/08/us/university-o...   \n",
       "1  https://www.cnn.com/2023/12/08/europe/azerbaij...   \n",
       "2  https://www.cnn.com/2023/12/08/china/italy-bel...   \n",
       "3  https://www.cnn.com/2023/12/08/politics/congre...   \n",
       "4  https://www.cnn.com/style/stella-mccartney-lea...   \n",
       "\n",
       "                                            imageURL  \\\n",
       "0  https://media.cnn.com/api/v1/images/stellar/pr...   \n",
       "1  https://media.cnn.com/api/v1/images/stellar/pr...   \n",
       "2  https://media.cnn.com/api/v1/images/stellar/pr...   \n",
       "3  https://media.cnn.com/api/v1/images/stellar/pr...   \n",
       "4  https://media.cnn.com/api/v1/images/stellar/pr...   \n",
       "\n",
       "                     cat_label  \\\n",
       "0                     Politics   \n",
       "1           International News   \n",
       "2           International News   \n",
       "3                     Politics   \n",
       "4  Entertainment and Lifestyle   \n",
       "\n",
       "                                           sourceURL  \\\n",
       "0  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "1  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "2  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "3  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "4  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "\n",
       "                                          clean_blob  \n",
       "0  police country slain return relative patricia ...  \n",
       "1  country european climate prosperous export enc...  \n",
       "2  country european ursula raise reason refer top...  \n",
       "3  country philosophical raise spring anna increa...  \n",
       "4  climate showcased available enter combination ...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"cat_label\"].isnull(), \"cat_label\"] = df.loc[df[\"cat_label\"].isnull(), \"pred_cat\"]\n",
    "df.drop([\"Category\", \"pred_cat\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3a060",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59edc42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text lenght</th>\n",
       "      <th>URL</th>\n",
       "      <th>imageURL</th>\n",
       "      <th>cat_label</th>\n",
       "      <th>sourceURL</th>\n",
       "      <th>clean_blob</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Details emerge about UNLV gunman who killed 3 ...</td>\n",
       "      <td>2023-12-08 03:06:00</td>\n",
       "      <td>Elizabeth Wolfe</td>\n",
       "      <td>Investigators searching for the motive of Anth...</td>\n",
       "      <td>3999</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/us/university-o...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>police country slain return relative patricia ...</td>\n",
       "      <td>-0.98715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Azerbaijan and Armenia agree to prisoner swap ...</td>\n",
       "      <td>2023-12-08 02:44:00</td>\n",
       "      <td>Angela Dewan</td>\n",
       "      <td>Azerbaijan and Armenia have agreed to a prison...</td>\n",
       "      <td>3408</td>\n",
       "      <td>https://www.cnn.com/2023/12/08/europe/azerbaij...</td>\n",
       "      <td>https://media.cnn.com/api/v1/images/stellar/pr...</td>\n",
       "      <td>International News</td>\n",
       "      <td>https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...</td>\n",
       "      <td>country european climate prosperous export enc...</td>\n",
       "      <td>0.94895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source                                              Title  \\\n",
       "0    CNN  Details emerge about UNLV gunman who killed 3 ...   \n",
       "1    CNN  Azerbaijan and Armenia agree to prisoner swap ...   \n",
       "\n",
       "                 Date           Author  \\\n",
       "0 2023-12-08 03:06:00  Elizabeth Wolfe   \n",
       "1 2023-12-08 02:44:00     Angela Dewan   \n",
       "\n",
       "                                                Text  Text lenght  \\\n",
       "0  Investigators searching for the motive of Anth...         3999   \n",
       "1  Azerbaijan and Armenia have agreed to a prison...         3408   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.cnn.com/2023/12/08/us/university-o...   \n",
       "1  https://www.cnn.com/2023/12/08/europe/azerbaij...   \n",
       "\n",
       "                                            imageURL           cat_label  \\\n",
       "0  https://media.cnn.com/api/v1/images/stellar/pr...            Politics   \n",
       "1  https://media.cnn.com/api/v1/images/stellar/pr...  International News   \n",
       "\n",
       "                                           sourceURL  \\\n",
       "0  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "1  https://tse2.mm.bing.net/th?id=OIP.W3gWO3a5Tnx...   \n",
       "\n",
       "                                          clean_blob  sentiment_score  \n",
       "0  police country slain return relative patricia ...         -0.98715  \n",
       "1  country european climate prosperous export enc...          0.94895  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment score\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['sentiment_scores1'] = df['clean_blob'].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "df['sentiment_scores2'] = df['Text'].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "df[\"sentiment_score\"] = (df['sentiment_scores1']+df['sentiment_scores2'])/2\n",
    "df.drop([\"sentiment_scores1\", \"sentiment_scores2\"], axis=1, inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c409b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensationalism score\n",
    "\n",
    "sensationalism_keywords = ['breaking', 'exclusive', 'shocking', 'explosive', 'revealed', 'urgent', 'unbelievable', 'mind-blowing', 'scandalous', 'outrageous', 'sensational', 'never-before-seen', 'dramatic', 'jaw-dropping', 'killer', 'massive', 'insane', 'terrifying', 'banned', 'controversial', 'secret', 'conspiracy', 'nightmare', 'apocalyptic', 'incredible', 'forbidden', 'sinister', 'catastrophic', 'shock', 'danger', 'fear', 'panic', 'monster', 'death-defying', 'omg', 'alarming', 'tremendous', 'never', 'deadly', 'hellish', 'paranormal', 'bewildering', 'menacing', 'twisted', 'kill', 'wild', 'devious', 'exposed', 'unprecedented', 'crisis', 'apocalypse', 'bizarre', 'explosive revelation', 'apocalyptic', 'deadly virus', 'unearthed', 'censored', 'life-threatening', 'cataclysmic', 'underground', 'illegal', 'menace', 'gruesome', 'intense', 'unholy', 'insidious', 'doomsday', 'untold', 'mysterious', 'censored', 'horrifying', 'unexplained', 'phenomenal', 'surreal', 'freakish', 'clash', 'supernatural', 'reckless', 'banned', 'taboo', 'untamed', 'monstrous', 'forbidden', 'pandemonium', 'infernal', 'no holds barred', 'ruthless', 'ghostly', 'frightening', 'dangerous liaison', 'freak accident', 'aberration', 'anarchy', 'untouchable', 'eerie', 'conspiracy theory', 'lost civilization', 'apocalyptic nightmare', 'inexplicable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a6308e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensationalism_score(x):\n",
    "    sensationalism_score = 0  # base score\n",
    "    x_lower = x.lower()  \n",
    "\n",
    "    for keyword in sensationalism_keywords:\n",
    "        keyword_count = x_lower.count(keyword.lower()) \n",
    "        sensationalism_score += 0.3 * keyword_count  # Add to score based on keyword occurrences\n",
    "\n",
    "    divisor = len(x) / 1000  \n",
    "    sensationalism_score /= divisor  # Normalize the score with dividing by text length\n",
    "    return sensationalism_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d86b6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sensationalism_score\"] = df[\"Text\"].apply(sensationalism_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d40a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# readability score\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4a197b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coleman_liau_index(x):\n",
    "    words = word_tokenize(x)\n",
    "    words_count = len(words)\n",
    "    sentences = sent_tokenize(x)\n",
    "    sentences = len(sentences)\n",
    "    letters = sum(len(letter) for word in words for letter in word)\n",
    "    L = (letters / words_count) * 100\n",
    "    S = (sentences / words_count) * 100\n",
    "    return 0.0588 * L - 0.296 * S - 15.8\n",
    "df[\"Readability\"] = df[\"Text\"].apply(coleman_liau_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702a3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "def calculate_bias_score(text):\n",
    "    # Use TextBlob for sentiment analysis\n",
    "    analysis = TextBlob(text)\n",
    "    sentiment_score = analysis.sentiment.subjectivity\n",
    "    bias_score = sentiment_score\n",
    "    bias_score = np.sqrt(bias_score**2)\n",
    "    return bias_score\n",
    "df[\"bias_score\"] = df[\"Text\"].apply(calculate_bias_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d869f03",
   "metadata": {},
   "source": [
    "## Clickbait prediction | not finished yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bow_vect_clickbait.pkl', 'rb') as file:\n",
    "    bow_vect_clickbait = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b610d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clickbait_model.pkl', 'rb') as file:\n",
    "    clickbait_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6707e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_lenght\"] = df[\"Title\"].apply(lambda x: len(x))   # characters of title\n",
    "df[\"uppercased\"] = df[\"Title\"].apply(lambda x: len([l for l in x if l.isupper()])) # num of uppercased\n",
    "features = df[[\"title_lenght\", \"uppercased\"]]\n",
    "features = features.reset_index(drop=True)\n",
    "df.drop([\"title_lenght\", \"uppercased\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a322d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vect_clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca79d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = df[\"Title\"].reset_index(drop=True)\n",
    "vs = bow_vect_clickbait.transform(headlines).toarray()\n",
    "vs = pd.DataFrame(vs, columns=bow_vect_clickbait.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f73967",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([features, vs], axis=1)\n",
    "df[\"isClickbait\"] = clickbait_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"articles-tuesday-morning.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bed687",
   "metadata": {},
   "source": [
    "## TO SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53026ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = pd.read_csv(\"./datasets/all_articles.csv\")\n",
    "full_df = pd.concat([df, old_df], ignore_index=True)\n",
    "full_df.drop_duplicates(subset=\"URL\",keep=\"first\", inplace=True)\n",
    "full_df.drop_duplicates(subset=\"Title\", keep=\"first\", inplace=True)\n",
    "try:\n",
    "    full_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "except KeyError:\n",
    "    full_df = full_df\n",
    "    \n",
    "full_df.to_csv(\"./datasets/all_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d38ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.reset_index()\n",
    "full_df.rename(columns={'index': 'Article_ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2256e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = full_df.copy()\n",
    "to_drop = [\"Author\", \"Text\", \"Text lenght\", \"cat_label\", \"clean_blob\"]\n",
    "for i in to_drop:\n",
    "    articles.drop(i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19088fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = full_df.copy()\n",
    "to_drop = [\"Author\", \"Source\", \"Title\", \"Date\", \"URL\", \"cat_label\", \"sentiment_score\", \"sensationalism_score\", \"sourceURL\", \"imageURL\", \"Readability\", \"bias_score\", \"isClickbait\"]\n",
    "for i in to_drop:\n",
    "    text.drop(i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6125ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Author\":list(full_df[\"Author\"].unique())}\n",
    "authors = pd.DataFrame(data)\n",
    "authors.reset_index(inplace=True)\n",
    "authors.columns = [\"Author_ID\", \"Author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_author = pd.merge(authors, full_df, on=\"Author\", how=\"inner\")\n",
    "to_drop = [\"Source\", \"Title\", \"Date\", \"URL\", \"cat_label\", \"Text\", \"Text lenght\", \"clean_blob\", \"sentiment_score\",\"sourceURL\", \"imageURL\", \"sensationalism_score\", \"Readability\", \"bias_score\", \"isClickbait\"]\n",
    "for i in to_drop:\n",
    "    article_author.drop(i, axis=1, inplace=True)\n",
    "article_author = article_author.set_index(\"Author_ID\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dfb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = {\"Category\":list(full_df[\"cat_label\"].unique())}\n",
    "categories = pd.DataFrame(datas)\n",
    "categories.reset_index(inplace=True)\n",
    "categories.columns = [\"Category_ID\", \"Category\"]\n",
    "categories = categories[categories[\"Category\"]!=\"Breaking News\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_category = pd.merge(categories, full_df, left_on=\"Category\",right_on=\"cat_label\", how=\"inner\")\n",
    "to_drop = [\"Source\", \"Title\", \"Author\", \"Date\", \"URL\", \"cat_label\", \"Text\", \"Text lenght\", \"clean_blob\", \"sourceURL\", \"imageURL\",\"sentiment_score\", \"sensationalism_score\", \"Readability\", \"bias_score\"]\n",
    "for i in to_drop:\n",
    "    article_category.drop(i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4313e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = {\"Source\":list(full_df[\"Source\"].unique()), \"sourceURL\":list(full_df[\"sourceURL\"].unique())}\n",
    "source = pd.DataFrame(datas)\n",
    "source.reset_index(inplace=True)\n",
    "source.columns = [\"source_id\", \"Source\", \"sourceURL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source = pd.merge( source, full_df, on=\"Source\", how=\"inner\")\n",
    "to_drop = [\"Title\", \"Author\", \"Date\", \"URL\", \"cat_label\", \"Text\", \"Text lenght\", \"clean_blob\", \"sentiment_score\", \"sensationalism_score\", \"Readability\", \"imageURL\",\"bias_score\", \"isClickbait\"]\n",
    "for i in to_drop:\n",
    "    article_source.drop(i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd2c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "from getpass import getpass\n",
    "password = getpass()\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"root\",\n",
    "                               pw=password,\n",
    "                               db=\"headlinehub\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4edcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_sql(\"articles\",con = engine, if_exists = 'replace', chunksize = 1000,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.to_sql(\"text\",con = engine, if_exists = 'replace', chunksize = 1000,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da384f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.to_sql(\"author\",con = engine, if_exists = 'replace', chunksize = 1000,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_author.to_sql(\"article_author\",con = engine, if_exists = 'replace', chunksize = 1000,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e76128",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.to_sql(\"category\",con = engine, if_exists = 'replace', chunksize = 1000,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7dd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_category.to_sql(\"article_category\",con = engine, if_exists = 'replace', chunksize = 1000,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source.to_sql(\"source\",con = engine, if_exists = 'replace', chunksize = 1000,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765fe163",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source.to_sql(\"article_source\",con = engine, if_exists = 'replace', chunksize = 1000,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da3e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
